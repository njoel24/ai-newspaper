use rag
use mcp server
do fine tuning and publish the model
use stencil for web compoennts
use mdule federation for the microsfrnt end at run tim
extract shared deps
configure so I can easily switch model
huggingface and langchain
run in hodoku
using azure function and webhook

create a vector databaase and sudolang
Canvas and gum road
Ramalama ond oci conrtainer
Make it pluggable , so other people can add
Mrmc mode
Add nginix and express and security context
infra as code
typescript and ESM everywhere
Hugging Face inference, llama.cpp, or a LangChain pipeline
ephimeral env
use puppettee or playwright
Github actions
carbon.js
custom agents and mcp server from top right in vscode

During earlier runs we hit jsdom/parse5 ESM issues due to the environment and Node version. To avoid that, the test config uses the Node environment (in vitest.config.js) so tests run reliably. If you want full DOM testing (jsdom) later, upgrade Node to >=20.19.0 (or >=22.12.0) and switch environment to jsdom.
package.json was updated by npm to reflect installed devDependency versions.

ollama, lm studio, llama.cpp, localAi, vLLM 
let's also use workspaces and github actions
ngnix
process.env


// inline style, network request chain, reintroduce web-component ,
externals vs peerdeps
native app
lib ode vs app mode
CHECK PERFORMANCE TAB